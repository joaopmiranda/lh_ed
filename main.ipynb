{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-11 14:15:50,679 - INFO - Connecting to the Postgres database...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import warnings\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Configuration\n",
    "postgres_connection_string = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"database\": \"northwind\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"thewindisblowing\",\n",
    "    \"port\": 5432\n",
    "}\n",
    "\n",
    "\n",
    "csv_file_path = '.\\data\\order_details.csv'\n",
    "output_directory = '.\\data'\n",
    "\n",
    "# Connect to the Postgres database\n",
    "logging.info('Connecting to the Postgres database...')\n",
    "conn = psycopg2.connect(**postgres_connection_string)\n",
    "cursor = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-11 14:15:51,129 - INFO - Listing tables...\n",
      "2023-07-11 14:15:51,141 - INFO - Tables found:\n",
      "2023-07-11 14:15:51,145 - INFO - us_states\n",
      "2023-07-11 14:15:51,146 - INFO - customers\n",
      "2023-07-11 14:15:51,147 - INFO - orders\n",
      "2023-07-11 14:15:51,148 - INFO - employees\n",
      "2023-07-11 14:15:51,149 - INFO - shippers\n",
      "2023-07-11 14:15:51,150 - INFO - categories\n",
      "2023-07-11 14:15:51,151 - INFO - products\n",
      "2023-07-11 14:15:51,152 - INFO - suppliers\n",
      "2023-07-11 14:15:51,153 - INFO - region\n",
      "2023-07-11 14:15:51,154 - INFO - territories\n",
      "2023-07-11 14:15:51,155 - INFO - employee_territories\n",
      "2023-07-11 14:15:51,156 - INFO - customer_demographics\n",
      "2023-07-11 14:15:51,157 - INFO - customer_customer_demo\n"
     ]
    }
   ],
   "source": [
    "# List tables\n",
    "logging.info('Listing tables...')\n",
    "query = \"SELECT table_name FROM information_schema.tables WHERE table_schema = 'public'\"\n",
    "cursor.execute(query)\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "# Display the tables found\n",
    "logging.info('Tables found:')\n",
    "for table in tables:\n",
    "    logging.info(table[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-11 14:15:51,180 - INFO - Extracting data from the table: us_states\n",
      "2023-07-11 14:15:51,197 - INFO - Data from the table us_states extracted and saved to .\\data\\postgres\\us_states\\2021-01-01\\us_states.csv\n",
      "2023-07-11 14:15:51,201 - INFO - Extracting data from the table: customers\n",
      "2023-07-11 14:15:51,213 - INFO - Data from the table customers extracted and saved to .\\data\\postgres\\customers\\2021-01-01\\customers.csv\n",
      "2023-07-11 14:15:51,218 - INFO - Extracting data from the table: orders\n",
      "2023-07-11 14:15:51,245 - INFO - Data from the table orders extracted and saved to .\\data\\postgres\\orders\\2021-01-01\\orders.csv\n",
      "2023-07-11 14:15:51,249 - INFO - Extracting data from the table: employees\n",
      "2023-07-11 14:15:51,260 - INFO - Data from the table employees extracted and saved to .\\data\\postgres\\employees\\2021-01-01\\employees.csv\n",
      "2023-07-11 14:15:51,261 - INFO - Extracting data from the table: shippers\n",
      "2023-07-11 14:15:51,274 - INFO - Data from the table shippers extracted and saved to .\\data\\postgres\\shippers\\2021-01-01\\shippers.csv\n",
      "2023-07-11 14:15:51,276 - INFO - Extracting data from the table: categories\n",
      "2023-07-11 14:15:51,286 - INFO - Data from the table categories extracted and saved to .\\data\\postgres\\categories\\2021-01-01\\categories.csv\n",
      "2023-07-11 14:15:51,287 - INFO - Extracting data from the table: products\n",
      "2023-07-11 14:15:51,303 - INFO - Data from the table products extracted and saved to .\\data\\postgres\\products\\2021-01-01\\products.csv\n",
      "2023-07-11 14:15:51,304 - INFO - Extracting data from the table: suppliers\n",
      "2023-07-11 14:15:51,319 - INFO - Data from the table suppliers extracted and saved to .\\data\\postgres\\suppliers\\2021-01-01\\suppliers.csv\n",
      "2023-07-11 14:15:51,320 - INFO - Extracting data from the table: region\n",
      "2023-07-11 14:15:51,328 - INFO - Data from the table region extracted and saved to .\\data\\postgres\\region\\2021-01-01\\region.csv\n",
      "2023-07-11 14:15:51,331 - INFO - Extracting data from the table: territories\n",
      "2023-07-11 14:15:51,338 - INFO - Data from the table territories extracted and saved to .\\data\\postgres\\territories\\2021-01-01\\territories.csv\n",
      "2023-07-11 14:15:51,339 - INFO - Extracting data from the table: employee_territories\n",
      "2023-07-11 14:15:51,349 - INFO - Data from the table employee_territories extracted and saved to .\\data\\postgres\\employee_territories\\2021-01-01\\employee_territories.csv\n",
      "2023-07-11 14:15:51,350 - INFO - Extracting data from the table: customer_demographics\n",
      "2023-07-11 14:15:51,356 - INFO - Data from the table customer_demographics extracted and saved to .\\data\\postgres\\customer_demographics\\2021-01-01\\customer_demographics.csv\n",
      "2023-07-11 14:15:51,358 - INFO - Extracting data from the table: customer_customer_demo\n",
      "2023-07-11 14:15:51,372 - INFO - Data from the table customer_customer_demo extracted and saved to .\\data\\postgres\\customer_customer_demo\\2021-01-01\\customer_customer_demo.csv\n"
     ]
    }
   ],
   "source": [
    "# List of tables to extract\n",
    "tables = [\n",
    "    'us_states',\n",
    "    'customers',\n",
    "    'orders',\n",
    "    'employees',\n",
    "    'shippers',\n",
    "    'categories',\n",
    "    'products',\n",
    "    'suppliers',\n",
    "    'region',\n",
    "    'territories',\n",
    "    'employee_territories',\n",
    "    'customer_demographics',\n",
    "    'customer_customer_demo'\n",
    "]  \n",
    "\n",
    "execution_day = '2021-01-01'  # Replace with the desired execution day\n",
    "\n",
    "for table in tables:\n",
    "    logging.info(f'Extracting data from the table: {table}')\n",
    "    query = f'SELECT * FROM {table};'\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    output_path = os.path.join(output_directory, \"postgres\", table, execution_day, f'{table}.csv')\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    logging.info(f'Data from the table {table} extracted and saved to {output_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-11 14:15:51,457 - INFO - Checking table integrity...\n",
      "2023-07-11 14:15:51,464 - INFO - Table us_states has 51 rows.\n",
      "2023-07-11 14:15:51,467 - INFO - Table customers has 91 rows.\n",
      "2023-07-11 14:15:51,468 - INFO - Table orders has 830 rows.\n",
      "2023-07-11 14:15:51,470 - INFO - Table employees has 9 rows.\n",
      "2023-07-11 14:15:51,471 - INFO - Table shippers has 6 rows.\n",
      "2023-07-11 14:15:51,473 - INFO - Table categories has 8 rows.\n",
      "2023-07-11 14:15:51,475 - INFO - Table products has 77 rows.\n",
      "2023-07-11 14:15:51,478 - INFO - Table suppliers has 29 rows.\n",
      "2023-07-11 14:15:51,479 - INFO - Table region has 4 rows.\n",
      "2023-07-11 14:15:51,481 - INFO - Table territories has 53 rows.\n",
      "2023-07-11 14:15:51,482 - INFO - Table employee_territories has 49 rows.\n",
      "2023-07-11 14:15:51,484 - INFO - Table customer_demographics has 0 rows.\n",
      "2023-07-11 14:15:51,485 - INFO - Table customer_customer_demo has 0 rows.\n"
     ]
    }
   ],
   "source": [
    "# Check integrity of each table\n",
    "logging.info('Checking table integrity...')\n",
    "for table in tables:\n",
    "    query = f\"SELECT COUNT(*) FROM {table}\"\n",
    "    cursor.execute(query)\n",
    "    result = cursor.fetchone()\n",
    "    count = result[0]\n",
    "    logging.info(f\"Table {table} has {count} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-11 14:15:51,552 - INFO - Extracting data from the CSV file...\n",
      "2023-07-11 14:15:51,591 - INFO - Data from the CSV file extracted and saved to .\\data\\csv\\2021-01-01\\orders_details.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract data from the CSV file\n",
    "logging.info('Extracting data from the CSV file...')\n",
    "df = pd.read_csv(csv_file_path)\n",
    "output_path = os.path.join(output_directory, 'csv', execution_day, 'orders_details.csv')\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "df.to_csv(output_path, index=False)\n",
    "logging.info(f'Data from the CSV file extracted and saved to {output_path}')\n",
    "\n",
    "# Close the Postgres connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-11 14:15:51,622 - INFO - Connecting to the final database...\n"
     ]
    }
   ],
   "source": [
    "# Load data from the local disk into the final database\n",
    "logging.info('Connecting to the final database...')\n",
    "final_database_connection_string = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'northwind_v2',\n",
    "    'user': 'postgres',\n",
    "    'password': '123456'\n",
    "}\n",
    "\n",
    "# Connect to the final database\n",
    "conn = create_engine('postgresql://postgres:123456@localhost/northwind_v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-11 14:15:52,455 - INFO - Data from us_states loaded into the final database\n",
      "2023-07-11 14:15:52,540 - INFO - Data from customers loaded into the final database\n",
      "2023-07-11 14:15:52,707 - INFO - Data from orders loaded into the final database\n",
      "2023-07-11 14:15:52,782 - INFO - Data from employees loaded into the final database\n",
      "2023-07-11 14:15:52,843 - INFO - Data from shippers loaded into the final database\n",
      "2023-07-11 14:15:52,907 - INFO - Data from categories loaded into the final database\n",
      "2023-07-11 14:15:52,992 - INFO - Data from products loaded into the final database\n",
      "2023-07-11 14:15:53,070 - INFO - Data from suppliers loaded into the final database\n",
      "2023-07-11 14:15:53,140 - INFO - Data from region loaded into the final database\n",
      "2023-07-11 14:15:53,207 - INFO - Data from territories loaded into the final database\n",
      "2023-07-11 14:15:53,262 - INFO - Data from employee_territories loaded into the final database\n",
      "2023-07-11 14:15:53,347 - INFO - Data from customer_demographics loaded into the final database\n",
      "2023-07-11 14:15:53,513 - INFO - Data from customer_customer_demo loaded into the final database\n"
     ]
    }
   ],
   "source": [
    "# Load data from the Postgres extracts\n",
    "postgres_extract_directory = os.path.join(output_directory, 'postgres', execution_day)\n",
    "for table in tables:\n",
    "    extract_file_path = os.path.join(output_directory, 'postgres', table, execution_day, f'{table}.csv')\n",
    "    df = pd.read_csv(extract_file_path)\n",
    "    df.to_sql(table, conn, if_exists='replace', index=False)\n",
    "    logging.info(f'Data from {table} loaded into the final database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-11 14:15:53,709 - INFO - Data from CSV loaded into the final database\n"
     ]
    }
   ],
   "source": [
    "# Load data from the CSV extract\n",
    "csv_extract_directory = os.path.join(output_directory, 'csv', execution_day)\n",
    "csv_extract_file_path = os.path.join(csv_extract_directory, 'orders_details.csv')\n",
    "df = pd.read_csv(csv_extract_file_path)\n",
    "df.to_sql('order_details', conn, if_exists='replace', index=False)\n",
    "logging.info('Data from CSV loaded into the final database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-11 14:15:53,743 - INFO - Running the query to retrieve orders and details...\n",
      "2023-07-11 14:15:53,850 - INFO - Final query result saved to .\\data\\final_result\\2021-01-01\\final_result.csv\n",
      "2023-07-11 14:15:53,852 - INFO - Pipeline completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Run a query to retrieve the orders and their details from the final database\n",
    "logging.info('Running the query to retrieve orders and details...')\n",
    "final_query = \"\"\"\n",
    "SELECT o.*, od.*\n",
    "FROM orders o\n",
    "JOIN order_details od ON o.order_id = od.order_id;\n",
    "\"\"\"\n",
    "output_file_path = os.path.join(output_directory, 'final_result', execution_day)\n",
    "os.makedirs(os.path.join(output_file_path), exist_ok=True)\n",
    "output_file_path = os.path.join(output_file_path, \"final_result.csv\")\n",
    "\n",
    "df = pd.read_sql_query(final_query, conn)\n",
    "df.to_csv(output_file_path, index=False)\n",
    "logging.info(f'Final query result saved to {output_file_path}')\n",
    "\n",
    "logging.info('Pipeline completed successfully.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
